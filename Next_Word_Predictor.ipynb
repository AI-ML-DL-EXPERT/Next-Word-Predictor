{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1UxusA-XxrZ_CDigdLGGs6CpD6O6rBcCV",
      "authorship_tag": "ABX9TyOig7mK8n6FR2WYIrIIYv1L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-ML-DL-EXPERT/Next-Word-Predictor/blob/main/Next_Word_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qjGX_Nt37GZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import string\n",
        "\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "    # Open the file\n",
        "    file = open(filename, \"r\")\n",
        "\n",
        "    # Read all text\n",
        "    text = file.read()\n",
        "\n",
        "    # Close the file\n",
        "    file.close()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "aWGaWUV0pgDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Document\n",
        "filename = \"/content/drive/MyDrive/Deep_Learning_Datasets/republic_clean.txt\"\n",
        "\n",
        "doc = load_doc(filename)\n",
        "\n",
        "print(doc[: 200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpf-SMWUqVUg",
        "outputId": "f4be39fb-e9f2-422b-ae34-921fd12e95ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ï»¿BOOK I.\n",
            "\n",
            "\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
            "Artemis.); and also because I wanted to see in wh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # Replace \"--\" with a space \" \"\n",
        "    doc = doc.replace(\"--\", \" \")\n",
        "\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "\n",
        "    # Remove punctuation from each token\n",
        "    \"\"\"The `str.maketrans(\"\", \"\", string.punctuation)` function creates a\n",
        "    translation table that can be used to remove punctuation from a string.\n",
        "     It takes three arguments: the characters to be replaced, the characters\n",
        "     to replace them with, and the characters to delete. In this case, it\n",
        "     removes all punctuation from the string.\"\"\"\n",
        "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "\n",
        "    # Remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "    # Make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "_ibWAQWSqq5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Document\n",
        "tokens = clean_doc(doc)\n",
        "\n",
        "print(tokens[: 200])\n",
        "print(\"Total tokens: \", len(tokens))\n",
        "print(\"Unique tokens: \", len(set(tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crDo56DxwRTa",
        "outputId": "e3db2f58-bdef-4a4b-f11e-7b515d9f602a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said', 'to']\n",
            "Total tokens:  118683\n",
            "Unique tokens:  7409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Clean Text\n",
        "\n",
        "# Organize into sequence of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "\n",
        "for i in range(length, len(tokens)):\n",
        "    # Select sequence of tokens\n",
        "    seq = tokens[i - length: i]\n",
        "\n",
        "    # Convert into a line\n",
        "\n",
        "    line = ' '.join(seq)\n",
        "\n",
        "    # Store\n",
        "    sequences.append(line)\n",
        "\n",
        "print(\"Total sequences: \", len(sequences))\n",
        "\n",
        "sequences[: 10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gWcF46nwUSS",
        "outputId": "d97e3119-869b-4f24-e7f7-39dee27e1c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequences:  118632\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted',\n",
              " 'i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with',\n",
              " 'went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the',\n",
              " 'down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession',\n",
              " 'yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of',\n",
              " 'to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of the',\n",
              " 'the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of the inhabitants',\n",
              " 'piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of the inhabitants but',\n",
              " 'with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of the inhabitants but that',\n",
              " 'glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of the inhabitants but that of']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokens to file, one dialog per line\n",
        "\n",
        "def save_doc(lines, filename):\n",
        "    data = \"\\n\".join(lines)\n",
        "    file = open(filename, \"w\")\n",
        "    file.write(data)\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "jTRv5G-EwjX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save sequences to file\n",
        "\n",
        "out_filename = \"/content/drive/MyDrive/Deep_Learning_Datasets/republic_sequences.txt\"\n",
        "\n",
        "save_doc(sequences, out_filename)"
      ],
      "metadata": {
        "id": "-63rjQuHCblW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Sequences"
      ],
      "metadata": {
        "id": "12smoBdmR6Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the doc into memory\n",
        "\n",
        "in_filename = \"/content/drive/MyDrive/Deep_Learning_Datasets/republic_sequences.txt\"\n",
        "\n",
        "doc = load_doc(in_filename)\n",
        "\n",
        "lines = doc.split(\"\\n\")"
      ],
      "metadata": {
        "id": "JdXdYSn8CyXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQfDtgIYSW_g",
        "outputId": "00cd288a-3321-4da6-e053-b98f56803b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted',\n",
              " 'i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with',\n",
              " 'went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the',\n",
              " 'down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession',\n",
              " 'yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode Sequences"
      ],
      "metadata": {
        "id": "ntKJ8agsSezz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Integer encode sequences of words\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "metadata": {
        "id": "EzksqxJASY_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igCxgWHwS2yf",
        "outputId": "6eec07d1-9080-422c-c8f9-454ab0168ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7410"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate into input and output\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "x, y = sequences[:, : -1], sequences[:, -1]\n",
        "\n",
        "y = to_categorical(y, num_classes = vocab_size)\n",
        "\n",
        "seq_length = x.shape[1]"
      ],
      "metadata": {
        "id": "weKrbE2bS5P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0fquKMJVDG_",
        "outputId": "f5ee6f88-79b3-4d7c-b3af-ec223c313c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118632, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJQi5GxmWS5T",
        "outputId": "344da576-3add-496a-f510-1054ac54ea4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118632, 7410)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit Model"
      ],
      "metadata": {
        "id": "EZ33A94GWwLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length= seq_length))\n",
        "model.add(LSTM(100, return_sequences= True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation = \"relu\"))\n",
        "model.add(Dense(vocab_size, activation = \"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R4tv2lQWywA",
        "outputId": "58a0e126-0a8a-47c7-9dd2-98e36a1fe83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 50)            370500    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50, 100)           60400     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7410)              748410    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1269810 (4.84 MB)\n",
            "Trainable params: 1269810 (4.84 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile Model\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "\n",
        "# Fit model\n",
        "\n",
        "history = model.fit(x, y, batch_size= 128, epochs= 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yFojoPVX4FL",
        "outputId": "d434c331-9b74-4ab8-b66f-e1307a9c3840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "927/927 [==============================] - 51s 50ms/step - loss: 6.1504 - accuracy: 0.0743\n",
            "Epoch 2/100\n",
            "927/927 [==============================] - 20s 22ms/step - loss: 5.6793 - accuracy: 0.1081\n",
            "Epoch 3/100\n",
            "927/927 [==============================] - 17s 19ms/step - loss: 5.4291 - accuracy: 0.1319\n",
            "Epoch 4/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 5.2784 - accuracy: 0.1442\n",
            "Epoch 5/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 5.1678 - accuracy: 0.1521\n",
            "Epoch 6/100\n",
            "927/927 [==============================] - 16s 18ms/step - loss: 5.0751 - accuracy: 0.1583\n",
            "Epoch 7/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 4.9938 - accuracy: 0.1639\n",
            "Epoch 8/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 4.9182 - accuracy: 0.1695\n",
            "Epoch 9/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 4.8437 - accuracy: 0.1738\n",
            "Epoch 10/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.7744 - accuracy: 0.1770\n",
            "Epoch 11/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.7066 - accuracy: 0.1801\n",
            "Epoch 12/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 4.6409 - accuracy: 0.1838\n",
            "Epoch 13/100\n",
            "927/927 [==============================] - 15s 17ms/step - loss: 4.5810 - accuracy: 0.1863\n",
            "Epoch 14/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 4.5229 - accuracy: 0.1895\n",
            "Epoch 15/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 4.4693 - accuracy: 0.1914\n",
            "Epoch 16/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 4.4195 - accuracy: 0.1936\n",
            "Epoch 17/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 4.3721 - accuracy: 0.1962\n",
            "Epoch 18/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 4.3273 - accuracy: 0.1982\n",
            "Epoch 19/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 4.2833 - accuracy: 0.2006\n",
            "Epoch 20/100\n",
            "927/927 [==============================] - 15s 17ms/step - loss: 4.2444 - accuracy: 0.2025\n",
            "Epoch 21/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 4.2084 - accuracy: 0.2050\n",
            "Epoch 22/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 4.1731 - accuracy: 0.2070\n",
            "Epoch 23/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 4.1395 - accuracy: 0.2096\n",
            "Epoch 24/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 4.1083 - accuracy: 0.2127\n",
            "Epoch 25/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.0780 - accuracy: 0.2141\n",
            "Epoch 26/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 4.0476 - accuracy: 0.2164\n",
            "Epoch 27/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 4.0211 - accuracy: 0.2191\n",
            "Epoch 28/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.9924 - accuracy: 0.2217\n",
            "Epoch 29/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.9670 - accuracy: 0.2237\n",
            "Epoch 30/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.9440 - accuracy: 0.2255\n",
            "Epoch 31/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.9185 - accuracy: 0.2282\n",
            "Epoch 32/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.8939 - accuracy: 0.2306\n",
            "Epoch 33/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.8712 - accuracy: 0.2326\n",
            "Epoch 34/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 3.8483 - accuracy: 0.2354\n",
            "Epoch 35/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.8270 - accuracy: 0.2377\n",
            "Epoch 36/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.8050 - accuracy: 0.2396\n",
            "Epoch 37/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.7875 - accuracy: 0.2412\n",
            "Epoch 38/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.7635 - accuracy: 0.2442\n",
            "Epoch 39/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.7443 - accuracy: 0.2474\n",
            "Epoch 40/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.7236 - accuracy: 0.2488\n",
            "Epoch 41/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.7033 - accuracy: 0.2518\n",
            "Epoch 42/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.6853 - accuracy: 0.2540\n",
            "Epoch 43/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.6652 - accuracy: 0.2565\n",
            "Epoch 44/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.6467 - accuracy: 0.2583\n",
            "Epoch 45/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 3.6296 - accuracy: 0.2609\n",
            "Epoch 46/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.6113 - accuracy: 0.2631\n",
            "Epoch 47/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.5930 - accuracy: 0.2650\n",
            "Epoch 48/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 3.5756 - accuracy: 0.2674\n",
            "Epoch 49/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.5596 - accuracy: 0.2693\n",
            "Epoch 50/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.5413 - accuracy: 0.2709\n",
            "Epoch 51/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.5246 - accuracy: 0.2745\n",
            "Epoch 52/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 3.5070 - accuracy: 0.2765\n",
            "Epoch 53/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.4907 - accuracy: 0.2784\n",
            "Epoch 54/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.4746 - accuracy: 0.2808\n",
            "Epoch 55/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.4613 - accuracy: 0.2817\n",
            "Epoch 56/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.4408 - accuracy: 0.2857\n",
            "Epoch 57/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.4284 - accuracy: 0.2871\n",
            "Epoch 58/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 3.4138 - accuracy: 0.2893\n",
            "Epoch 59/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.4022 - accuracy: 0.2904\n",
            "Epoch 60/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 3.3788 - accuracy: 0.2945\n",
            "Epoch 61/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.3654 - accuracy: 0.2958\n",
            "Epoch 62/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.3525 - accuracy: 0.2974\n",
            "Epoch 63/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.3371 - accuracy: 0.3001\n",
            "Epoch 64/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.3209 - accuracy: 0.3025\n",
            "Epoch 65/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.3068 - accuracy: 0.3028\n",
            "Epoch 66/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.2933 - accuracy: 0.3064\n",
            "Epoch 67/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 3.2803 - accuracy: 0.3089\n",
            "Epoch 68/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.2634 - accuracy: 0.3097\n",
            "Epoch 69/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 3.2483 - accuracy: 0.3124\n",
            "Epoch 70/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.2355 - accuracy: 0.3141\n",
            "Epoch 71/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.2208 - accuracy: 0.3168\n",
            "Epoch 72/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.2092 - accuracy: 0.3176\n",
            "Epoch 73/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.1919 - accuracy: 0.3213\n",
            "Epoch 74/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.1827 - accuracy: 0.3224\n",
            "Epoch 75/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.1672 - accuracy: 0.3241\n",
            "Epoch 76/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.1538 - accuracy: 0.3264\n",
            "Epoch 77/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 3.1399 - accuracy: 0.3286\n",
            "Epoch 78/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.1232 - accuracy: 0.3314\n",
            "Epoch 79/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.1118 - accuracy: 0.3332\n",
            "Epoch 80/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.0978 - accuracy: 0.3360\n",
            "Epoch 81/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 3.0867 - accuracy: 0.3367\n",
            "Epoch 82/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.0717 - accuracy: 0.3397\n",
            "Epoch 83/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.0579 - accuracy: 0.3404\n",
            "Epoch 84/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 3.0443 - accuracy: 0.3429\n",
            "Epoch 85/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.0329 - accuracy: 0.3460\n",
            "Epoch 86/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.0218 - accuracy: 0.3477\n",
            "Epoch 87/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 3.0069 - accuracy: 0.3496\n",
            "Epoch 88/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.9945 - accuracy: 0.3515\n",
            "Epoch 89/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 2.9809 - accuracy: 0.3531\n",
            "Epoch 90/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 2.9699 - accuracy: 0.3557\n",
            "Epoch 91/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 2.9542 - accuracy: 0.3582\n",
            "Epoch 92/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 2.9404 - accuracy: 0.3601\n",
            "Epoch 93/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 2.9289 - accuracy: 0.3620\n",
            "Epoch 94/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 2.9180 - accuracy: 0.3643\n",
            "Epoch 95/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 2.9042 - accuracy: 0.3657\n",
            "Epoch 96/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 2.8930 - accuracy: 0.3672\n",
            "Epoch 97/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 2.8803 - accuracy: 0.3708\n",
            "Epoch 98/100\n",
            "927/927 [==============================] - 14s 16ms/step - loss: 2.8664 - accuracy: 0.3730\n",
            "Epoch 99/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 2.8548 - accuracy: 0.3742\n",
            "Epoch 100/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 2.8420 - accuracy: 0.3768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model"
      ],
      "metadata": {
        "id": "B-ygC3FzgKgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to file\n",
        "\n",
        "model.save(\"model.h5\")\n",
        "\n",
        "# Save the tokenizer\n",
        "\n",
        "pickle.dump(tokenizer, open(\"tokenizer.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "gf1wra9gYrmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "TKsDhIRWhQNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "\n",
        "loadModel_For_testing = keras.models.load_model(\"/content/model.h5\")"
      ],
      "metadata": {
        "id": "IBJqmAETgyJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "\n",
        "tokenizer = pickle.load(open(\"tokenizer.pkl\", \"rb\"))"
      ],
      "metadata": {
        "id": "CQ00W-oJhhIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Text"
      ],
      "metadata": {
        "id": "jcTxBZcHiLrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a seed text\n",
        "\n",
        "seed_text = lines[np.random.randint(0, len(lines))]\n",
        "\n",
        "print(seed_text + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq-y2lk8iIDp",
        "outputId": "707bbc89-9c28-4a35-e670-e3662ff52300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acquisition of knowledge of any kind bodily exercise when compulsory does no harm to the body but knowledge which is acquired under compulsion obtains no hold on the mind very true then my good friend i said do not use compulsion but let early education be a sort of amusement you\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.texts_to_sequences([seed_text])[0]"
      ],
      "metadata": {
        "id": "jPuS81JNiqaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gmfk-XLUkKJ6",
        "outputId": "61eee7eb-9be5-4324-e01c-6f864446510c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1157,\n",
              " 3,\n",
              " 104,\n",
              " 3,\n",
              " 45,\n",
              " 211,\n",
              " 492,\n",
              " 982,\n",
              " 40,\n",
              " 6382,\n",
              " 161,\n",
              " 47,\n",
              " 509,\n",
              " 4,\n",
              " 1,\n",
              " 171,\n",
              " 22,\n",
              " 104,\n",
              " 13,\n",
              " 5,\n",
              " 1050,\n",
              " 180,\n",
              " 2805,\n",
              " 3125,\n",
              " 47,\n",
              " 489,\n",
              " 65,\n",
              " 1,\n",
              " 159,\n",
              " 67,\n",
              " 38,\n",
              " 31,\n",
              " 92,\n",
              " 48,\n",
              " 182,\n",
              " 11,\n",
              " 19,\n",
              " 41,\n",
              " 12,\n",
              " 172,\n",
              " 2805,\n",
              " 22,\n",
              " 86,\n",
              " 1376,\n",
              " 245,\n",
              " 10,\n",
              " 8,\n",
              " 119,\n",
              " 3,\n",
              " 1534,\n",
              " 15]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Select a seed text\n",
        "text = lines[np.random.randint(0, len(lines))]\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "  # tokenize\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token_text = pad_sequences([token_text], maxlen=seq_length, padding='pre')\n",
        "  # predict\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)\n",
        "      time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk5Z_Ho4j5ro",
        "outputId": "752bbc26-375e-4531-9d72-fe681eba7e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 693ms/step\n",
            "principles about justice and honour which were taught us in childhood and under their parental authority we have been brought up obeying and honouring them that is true there are also opposite maxims and habits of pleasure which flatter and attract the soul but do not influence those of us who are\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "principles about justice and honour which were taught us in childhood and under their parental authority we have been brought up obeying and honouring them that is true there are also opposite maxims and habits of pleasure which flatter and attract the soul but do not influence those of us who are not\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "principles about justice and honour which were taught us in childhood and under their parental authority we have been brought up obeying and honouring them that is true there are also opposite maxims and habits of pleasure which flatter and attract the soul but do not influence those of us who are not harmed\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "principles about justice and honour which were taught us in childhood and under their parental authority we have been brought up obeying and honouring them that is true there are also opposite maxims and habits of pleasure which flatter and attract the soul but do not influence those of us who are not harmed is\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "principles about justice and honour which were taught us in childhood and under their parental authority we have been brought up obeying and honouring them that is true there are also opposite maxims and habits of pleasure which flatter and attract the soul but do not influence those of us who are not harmed is the\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "principles about justice and honour which were taught us in childhood and under their parental authority we have been brought up obeying and honouring them that is true there are also opposite maxims and habits of pleasure which flatter and attract the soul but do not influence those of us who are not harmed is the author\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "principles about justice and honour which were taught us in childhood and under their parental authority we have been brought up obeying and honouring them that is true there are also opposite maxims and habits of pleasure which flatter and attract the soul but do not influence those of us who are not harmed is the author of\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "principles about justice and honour which were taught us in childhood and under their parental authority we have been brought up obeying and honouring them that is true there are also opposite maxims and habits of pleasure which flatter and attract the soul but do not influence those of us who are not harmed is the author of the\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "principles about justice and honour which were taught us in childhood and under their parental authority we have been brought up obeying and honouring them that is true there are also opposite maxims and habits of pleasure which flatter and attract the soul but do not influence those of us who are not harmed is the author of the pregnant\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "principles about justice and honour which were taught us in childhood and under their parental authority we have been brought up obeying and honouring them that is true there are also opposite maxims and habits of pleasure which flatter and attract the soul but do not influence those of us who are not harmed is the author of the pregnant blazes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IYaR241EkR5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kynSxVi1sEOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lnCjaXtAsoaP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}